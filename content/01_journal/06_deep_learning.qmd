---
title: "Deep Learning"
author: "Philip Suskin"
---

# Libraries

```{r}
# Load from specific python version
Sys.setenv(RETICULATE_PYTHON = "C:/Users/psusk/AppData/Local/Programs/Python/Python39")

library(tidyverse)
library(keras)
library(lime)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)

churn_data_raw <- read_csv("../../WA_Fn-UseC_-Telco-Customer-Churn.csv")

glimpse(churn_data_raw)
```

## Remove Churn

```{r}
churn_data_tbl <- churn_data_raw %>%
  select(-customerID) %>%
  drop_na(TotalCharges) %>%
  select(Churn, everything())

head(churn_data_tbl)
```

# Train/test

```{r}
set.seed(1)
train_test_split <- initial_split(churn_data_tbl, prop = 0.8)
train_test_split

# Retrieve train and test sets
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
```

## Cut tenure

```{r}
churn_data_tbl %>% ggplot(aes(x = tenure)) + 
  geom_histogram(bins = 6, color = "white", fill =  "#2DC6D6") +
  labs(
    title = "Tenure Counts With Six Bins",
    x     = "tenure (month)"
  )

train_tbl %>%
    select(Churn, TotalCharges) %>%
    mutate(
        Churn = Churn %>% as.factor() %>% as.numeric(),
        LogTotalCharges = log(TotalCharges)
        ) %>%
    correlate() %>%
    focus(Churn) %>%
    fashion()
```

## One-hot encode

```{r}
churn_data_tbl %>% 
        pivot_longer(cols      = c(Contract, InternetService, MultipleLines, PaymentMethod), 
                     names_to  = "feature", 
                     values_to = "category") %>% 
        ggplot(aes(category)) +
          geom_bar(fill = "#2DC6D6") +
          facet_wrap(~ feature, scales = "free") +
          labs(
            title = "Features with multiple categories: Need to be one-hot encoded"
          ) +
          theme(axis.text.x = element_text(angle = 25, 
                                           hjust = 1))
```

## mean-center and scale

```{r}
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
    step_rm(Churn) %>% 
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep(data = train_tbl)
```

# Model

```{r}
x_train_tbl <- bake(rec_obj, new_data = train_tbl)
x_test_tbl  <- bake(rec_obj, new_data = test_tbl)

y_train_vec <- ifelse(train_tbl$Churn == "Yes", 1, 0)
y_test_vec  <- ifelse(test_tbl$Churn == "Yes", 1, 0)

model_keras <- keras_model_sequential()

model_keras %>% 
    # First hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu", 
        input_shape        = ncol(x_train_tbl)) %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Second hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu") %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Output layer
    layer_dense(
        units              = 1, 
        kernel_initializer = "uniform", 
        activation         = "sigmoid") %>% 
    # Compile ANN
    compile(
        optimizer = 'adam',
        loss      = 'binary_crossentropy',
        metrics   = c('accuracy')
    )
model_keras
```

# Fit

```{r}
x_train_matrix = as.matrix(x_train_tbl)
y_train_matrix = as.matrix(y_train_vec)
x_test_matrix = as.matrix(x_test_tbl)
y_test_matrix = as.matrix(y_test_vec)
fit_keras <-  fit(
    model_keras,
    x = x_train_matrix,
    y = y_train_matrix,
    epochs = 35,
    batch_size = 50,
    validation_data = list(x_test_matrix, y_test_matrix),
    validation_split = 0.3
  )
fit_keras
```

#Predictions

```{r}
yhat_keras_class_vec <- predict(model_keras, as.matrix(x_test_tbl)) %>%
  as.vector() %>%
  round()

yhat_keras_prob_vec  <- predict(model_keras, as.matrix(x_test_tbl)) %>%
    as.vector()
```

# Estimations

```{r}
estimates_keras_tbl <- tibble(
  truth = as.factor(y_test_vec) %>% fct_recode(Yes = "1", No = "0"),
  estimate = as.factor(yhat_keras_class_vec) %>% fct_recode(Yes = "1", No = "0"),
  class_prob = yhat_keras_prob_vec
)

estimates_keras_tbl
```

# Confusion Table

```{r}
confusion_table <- conf_mat(data = estimates_keras_tbl, truth, estimate)

confusion_table

accuracy_result <- accuracy(data = estimates_keras_tbl, truth, estimate)

accuracy_result

auc_result <- roc_auc(data = estimates_keras_tbl, truth, class_prob, event_level = "second")

auc_result
```

# Precision

```{r}
precision_result <- precision(data = estimates_keras_tbl, truth, estimate)

recall_result <- recall(data = estimates_keras_tbl, truth, estimate)

precision_recall_tbl <- tibble(
  precision = precision_result$.estimate,
  recall = recall_result$.estimate
)

precision_recall_tbl

estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
```

# LIME

```{r}
estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)

model_type.keras.engine.sequential.Sequential  <- function(x, ...) {
    return("classification")
}

predict_model.keras.engine.sequential.Sequential <- function(x, newdata, type, ...) {
    pred <- predict(object = x, x = as.matrix(newdata))
    return(data.frame(Yes = pred, No = 1 - pred))
}

predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%
    tibble::as_tibble()

explainer <- lime::lime(
  x = x_train_tbl,
  model = model_keras,
  bin_continuous = FALSE
)

x_test_df <- as.data.frame(x_test_tbl)

explanation <- lime::explain(
  x = x_test_df[1:10, ],
  explainer = explainer,
  n_labels = 2,
  n_features = 50
)

plot_features(explanation)
plot_explanations(explanation)
```

# Correlation Analysis

```{r}
corrr_analysis <- x_train_tbl %>%
    mutate(Churn = y_train_vec) %>%
    correlate() %>%
    focus(Churn) %>%
    rename(feature = term) %>%
    arrange(abs(Churn)) %>%
    mutate(feature = as_factor(feature)) 
corrr_analysis

corrr_analysis %>%
  ggplot(aes(x = Churn, y = fct_reorder(feature, desc(Churn)))) +
  geom_point() +
  
  geom_segment(aes(xend = 0, yend = feature), 
               color = "red", 
               data = corrr_analysis %>% filter(Churn > 0)) +
  geom_point(color = "red", 
             data = corrr_analysis %>% filter(Churn > 0)) +
  
  geom_segment(aes(xend = 0, yend = feature), 
               color = "#2DC6D6", 
               data = corrr_analysis %>% filter(Churn < 0)) +
  geom_point(color = "#2DC6D6", 
             data = corrr_analysis %>% filter(Churn < 0)) +
  
  geom_vline(xintercept = 0, color = "#f1fa8c", size = 1, linetype = 2) +
  geom_vline(xintercept = 0.25, color = "#f1fa8c", size = 0.5, linetype = 2) +
  geom_vline(xintercept = -0.25, color = "#f1fa8c", size = 0.5, linetype = 2) +
  
  labs(x = "Churn", y = "Feature Importance", title = "Churn correlation analysis", subtitle = "Positive Correlations (contribute to churn), Negative Correlations (prevent Churn)")
```