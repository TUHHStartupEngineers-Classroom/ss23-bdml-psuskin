{
  "hash": "315ccd6be5f6580fd31d1a5631151835",
  "result": {
    "markdown": "---\ntitle: \"Automated Machine Learning with H2O\"\nauthor: \"Philip Suskin\"\n---\n\n\n# Libraries\n\n\n::: {.cell hash='03_automated_ml_cache/html/unnamed-chunk-1_38229eed2b5402fad872bef3de00f183'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(h2o)\n\nlibrary(readxl)\nlibrary(GGally)\n\nlibrary(rsample)\nlibrary(recipes)\n```\n:::\n\n\n# Challenge 1\n\n# Data preparation\n\n\n::: {.cell hash='03_automated_ml_cache/html/unnamed-chunk-2_3e68810d89970d2b9c3716657d24ff2d'}\n\n```{.r .cell-code}\nemployee_attrition_tbl <- read_csv(\"../../HR-Employee-Attrition.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n    \n    color_expr <- enquo(color)\n    \n    if (rlang::quo_is_null(color_expr)) {\n        \n        g <- data %>%\n            ggpairs(lower = \"blank\") \n        \n    } else {\n        \n        color_name <- quo_name(color_expr)\n        \n        g <- data %>%\n            ggpairs(mapping = aes_string(color = color_name), \n                    lower = \"blank\", legend = 1,\n                    diag = list(continuous = wrap(\"densityDiag\", \n                                                  alpha = density_alpha))) +\n            theme(legend.position = \"bottom\")\n    }\n    \n    return(g)\n    \n}\n```\n:::\n\n::: {.cell hash='03_automated_ml_cache/html/unnamed-chunk-3_3d3d54fe391408c9d76029f2647bf3fe'}\n\n```{.r .cell-code}\n# Explore Features by Category\n\n#   1. Descriptive features: age, gender, marital status \nemployee_attrition_tbl %>%\n    select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n```\n:::\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#   2. Employment features: department, job role, job level\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %>%\n    plot_ggpairs(Attrition) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in cor(x, y): the standard deviation is zero\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n```\n:::\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#   4. Survey Results: Satisfaction level, WorkLifeBalance \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\n#   5. Performance Data: Job Involvment, Performance Rating\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-5.png){width=672}\n:::\n\n```{.r .cell-code}\n#   6. Work-Life Features \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-6.png){width=672}\n:::\n\n```{.r .cell-code}\n#   7. Training and Education \nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-7.png){width=672}\n:::\n\n```{.r .cell-code}\n#   8. Time-Based Features: Years at company, years in current role\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"years\")) %>%\n    plot_ggpairs(Attrition)\n```\n\n::: {.cell-output-display}\n![](03_automated_ml_files/figure-html/unnamed-chunk-3-8.png){width=672}\n:::\n:::\n\n\n# Analysis\n\n## Q1: What can you deduce about the interaction between Monthly Income and Attrition?\nc. Those that are leaving have a lower Monthly Income\n\n## Q2: What can you deduce about the interaction between Percent Salary Hike and Attrition?\nd. It's difficult to deduce anything based on the visualization\n\n## Q3: What can you deduce about the interaction between Stock Option Level and Attrition?\nb. Those that are staying have a higher stock option level\n\n## Q4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\na. A higher proportion of those leaving have a low environment satisfaction level\n\n## Q5: What can you deduce about the interaction between Work Life Balance and Attrition?\nb. Those that are staying have a higher density of 2's and 3's\n\n## Q6: What Can you deduce about the interaction between Job Involvement and Attrition?\na. Those that are leaving have a lower density of 3's and 4's\n\n## Q7: What can you deduce about the interaction between Over Time and Attrition?\na. The proportion of those leaving that are working Over Time are high compared to those that are not leaving\n\n## Q8: What can you deduce about the interaction between Training Times Last Year and Attrition?\nb. People that leave tend to have less annual trainings\n\n## Q9: What can you deduce about the interaction between Years At Company and Attrition?\nb. People that leave tend to have less working years at the company\n\n## Q10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\na. Those that are leaving have more years since last promotion than those that are staying\n\n# Challenge 2\n\n# Data preparation\n\n\n::: {.cell hash='03_automated_ml_cache/html/unnamed-chunk-4_e65f54e8ca9ff0538dc07bb03e8fe95a'}\n\n```{.r .cell-code}\nproduct_backorders_tbl <- read_csv(\"../../product_backorders.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Rows: 19053 Columns: 23\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nset.seed(1)\n\nsplit_obj <- rsample::initial_split(product_backorders_tbl, prop = 0.75)\n\ntrain_readable_tb1 <- training(split_obj)\ntest_readable_tb1  <- testing(split_obj)\n\nfactor_names <- c(\"went_on_backorder\")\nrecipie <- recipe(went_on_backorder ~ ., data = train_readable_tb1) %>%  \n           step_zv(all_predictors()) %>% \n           step_mutate_at(factor_names, fn = as.factor) %>%\n           step_center(all_numeric()) %>%\n           step_scale(all_numeric()) %>%\n           step_dummy(all_nominal(), -all_outcomes()) %>% \n           prep()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\n#> ℹ Please use `all_of()` or `any_of()` instead.\n#>   # Was:\n#>   data %>% select(factor_names)\n#> \n#>   # Now:\n#>   data %>% select(all_of(factor_names))\n#> \n#> See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n```\n:::\n\n```{.r .cell-code}\ntraining <- bake(recipie, new_data = train_readable_tb1)\ntesting  <- bake(recipie, new_data = test_readable_tb1)\n```\n:::\n\n\n# H2O\n\n\n::: {.cell hash='03_automated_ml_cache/html/unnamed-chunk-5_df960f98fe3d3a98efa302f949adda9a'}\n\n```{.r .cell-code}\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     C:\\Users\\psusk\\AppData\\Local\\Temp\\RtmpUXZTqh\\file319027832622/h2o_psusk_started_from_r.out\n#>     C:\\Users\\psusk\\AppData\\Local\\Temp\\RtmpUXZTqh\\file3190696f40f8/h2o_psusk_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting:  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 seconds 199 milliseconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 15 days \n#>     H2O cluster name:           H2O_started_from_R_psusk_wps853 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.54 GB \n#>     H2O cluster total cores:    16 \n#>     H2O cluster allowed cores:  16 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n```\n:::\n\n```{.r .cell-code}\nsplit_h2o <- h2o.splitFrame(as.h2o(training), ratios = c(0.75), seed = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(testing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n\nh2o_model <- h2o.automl(x = x,\n                        y = y,\n                        training_frame    = train_h2o,\n                        validation_frame  = valid_h2o,\n                        leaderboard_frame = test_h2o,\n                        max_runtime_secs  = 30,\n                        nfolds            = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n#> 07:16:05.678: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 07:16:05.693: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n\n```{.r .cell-code}\ntypeof(h2o_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"S4\"\n```\n:::\n\n```{.r .cell-code}\nslotNames(h2o_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n```\n:::\n\n```{.r .cell-code}\nh2o_model@leaderboard \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                                                 model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_1_AutoML_1_20230613_71605 0.9538722 0.1702684\n#> 2                          GBM_3_AutoML_1_20230613_71605 0.9522594 0.1787251\n#> 3                          GBM_4_AutoML_1_20230613_71605 0.9520314 0.1765737\n#> 4 StackedEnsemble_BestOfFamily_3_AutoML_1_20230613_71605 0.9503784 0.1757962\n#> 5 StackedEnsemble_BestOfFamily_2_AutoML_1_20230613_71605 0.9503128 0.1756711\n#> 6 StackedEnsemble_BestOfFamily_1_AutoML_1_20230613_71605 0.9497615 0.1733077\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7506365            0.1461145 0.2255489 0.05087230\n#> 2 0.7373834            0.1321909 0.2300257 0.05291181\n#> 3 0.7391740            0.1652463 0.2281500 0.05205244\n#> 4 0.7358277            0.1422044 0.2289516 0.05241884\n#> 5 0.7357762            0.1420854 0.2289590 0.05242220\n#> 6 0.7436516            0.1611713 0.2275031 0.05175764\n#> \n#> [13 rows x 7 columns]\n```\n:::\n\n```{.r .cell-code}\nh2o_model@leader \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_1_AutoML_1_20230613_71605 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              4/6\n#> 3      # GBM base models (used / total)              3/4\n#> 4      # DRF base models (used / total)              1/1\n#> 5      # GLM base models (used / total)              0/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.03523309\n#> RMSE:  0.1877048\n#> LogLoss:  0.1259497\n#> Mean Per-Class Error:  0.1199006\n#> AUC:  0.9797462\n#> AUCPR:  0.8898051\n#> Gini:  0.9594925\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8598  201 0.022844   =201/8799\n#> Yes     261  942 0.216958   =261/1203\n#> Totals 8859 1143 0.046191  =462/10002\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.435621    0.803069 167\n#> 2                       max f2  0.193206    0.851405 247\n#> 3                 max f0point5  0.607635    0.841584 119\n#> 4                 max accuracy  0.521416    0.954409 143\n#> 5                max precision  0.980591    1.000000   0\n#> 6                   max recall  0.016194    1.000000 366\n#> 7              max specificity  0.980591    1.000000   0\n#> 8             max absolute_mcc  0.435621    0.777251 167\n#> 9   max min_per_class_accuracy  0.187734    0.927681 249\n#> 10 max mean_per_class_accuracy  0.172192    0.929051 255\n#> 11                     max tns  0.980591 8799.000000   0\n#> 12                     max fns  0.980591 1201.000000   0\n#> 13                     max fps  0.001668 8799.000000 399\n#> 14                     max tps  0.016194 1203.000000 366\n#> 15                     max tnr  0.980591    1.000000   0\n#> 16                     max fnr  0.980591    0.998337   0\n#> 17                     max fpr  0.001668    1.000000 399\n#> 18                     max tpr  0.016194    1.000000 366\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.04895594\n#> RMSE:  0.2212599\n#> LogLoss:  0.1664229\n#> Mean Per-Class Error:  0.1491352\n#> AUC:  0.9546186\n#> AUCPR:  0.7448053\n#> Gini:  0.9092373\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2996 138 0.044033  =138/3134\n#> Yes     105 308 0.254237   =105/413\n#> Totals 3101 446 0.068509  =243/3547\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.397427    0.717113 173\n#> 2                       max f2  0.171032    0.789474 247\n#> 3                 max f0point5  0.551944    0.737705 129\n#> 4                 max accuracy  0.551944    0.935156 129\n#> 5                max precision  0.968180    1.000000   0\n#> 6                   max recall  0.006085    1.000000 387\n#> 7              max specificity  0.968180    1.000000   0\n#> 8             max absolute_mcc  0.397427    0.678853 173\n#> 9   max min_per_class_accuracy  0.163590    0.895022 250\n#> 10 max mean_per_class_accuracy  0.132888    0.899902 263\n#> 11                     max tns  0.968180 3134.000000   0\n#> 12                     max fns  0.968180  412.000000   0\n#> 13                     max fps  0.001534 3134.000000 399\n#> 14                     max tps  0.006085  413.000000 387\n#> 15                     max tnr  0.968180    1.000000   0\n#> 16                     max fnr  0.968180    0.997579   0\n#> 17                     max fpr  0.001534    1.000000 399\n#> 18                     max tpr  0.006085    1.000000 387\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05334899\n#> RMSE:  0.230974\n#> LogLoss:  0.1798809\n#> Mean Per-Class Error:  0.1545247\n#> AUC:  0.9460056\n#> AUCPR:  0.7267623\n#> Gini:  0.8920111\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8946  505 0.053433   =505/9451\n#> Yes     330  961 0.255616   =330/1291\n#> Totals 9276 1466 0.077732  =835/10742\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.308138    0.697135 209\n#> 2                       max f2  0.136668    0.767120 274\n#> 3                 max f0point5  0.530412    0.719342 141\n#> 4                 max accuracy  0.506068    0.928319 148\n#> 5                max precision  0.933395    0.927835  14\n#> 6                   max recall  0.001175    1.000000 398\n#> 7              max specificity  0.982676    0.999894   0\n#> 8             max absolute_mcc  0.308138    0.654488 209\n#> 9   max min_per_class_accuracy  0.112369    0.876309 286\n#> 10 max mean_per_class_accuracy  0.127459    0.880053 278\n#> 11                     max tns  0.982676 9450.000000   0\n#> 12                     max fns  0.982676 1291.000000   0\n#> 13                     max fps  0.000786 9451.000000 399\n#> 14                     max tps  0.001175 1291.000000 398\n#> 15                     max tnr  0.982676    0.999894   0\n#> 16                     max fnr  0.982676    1.000000   0\n#> 17                     max fpr  0.000786    1.000000 399\n#> 18                     max tpr  0.001175    1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.927681 0.003430   0.927671   0.923646   0.932616   0.925497\n#> auc         0.947049 0.006798   0.953948   0.951205   0.948346   0.936289\n#> err         0.072319 0.003430   0.072329   0.076354   0.067384   0.074503\n#> err_count 155.400000 8.142481 155.000000 165.000000 144.000000 161.000000\n#> f0point5    0.695976 0.018285   0.674074   0.717868   0.711790   0.689376\n#>           cv_5_valid\n#> accuracy    0.928972\n#> auc         0.945457\n#> err         0.071028\n#> err_count 152.000000\n#> f0point5    0.686773\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.691042  0.027322   0.657040   0.706790   0.724444\n#> r2                  0.494996  0.023577   0.509224   0.527480   0.485429\n#> recall              0.721367  0.053181   0.752066   0.765886   0.665306\n#> residual_deviance 770.734900 44.571510 713.016800 812.474100 750.668300\n#> rmse                0.230691  0.006382   0.221727   0.237345   0.228540\n#> specificity         0.955643  0.008115   0.950026   0.948980   0.967230\n#>                   cv_4_valid cv_5_valid\n#> precision           0.696721   0.670213\n#> r2                  0.466817   0.486029\n#> recall              0.661479   0.762097\n#> residual_deviance 818.929100 758.586500\n#> rmse                0.236365   0.229478\n#> specificity         0.961134   0.950846\n```\n:::\n\n```{.r .cell-code}\npred_tbl <- h2o.predict(h2o_model@leader, newdata = as.h2o(testing)) %>% \n            as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}